{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEA Coastlines validation\n",
    "\n",
    "To do:\n",
    "* [X] Change output CRS to Australian Albers\n",
    "* [X] Discard validation sides with multiple intersects?\n",
    "* [X] Split analysis code into:\n",
    "    * Aggregate multiple profiles and export into single file\n",
    "    * Analyse and plot single file\n",
    "* [X] Add extraction of environmental data for each profile line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnames = glob.glob('input_data/nswbpd/*.csv')\n",
    "# out = []\n",
    "\n",
    "# for fname in fnames:\n",
    "#     print(fname)\n",
    "#     df = pd.read_csv(fname, skiprows=5)\n",
    "#     out.append(df.Elevation < 0)\n",
    "# #     out.append(df.groupby(['Beach', 'Block', 'Profile', 'Year/Date'])['Elevation'].min() < 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules/functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "\n",
    "sys.path.append('/g/data/r78/DEACoastlines/')\n",
    "import deacoastlines_validation as deacl_val\n",
    "import deacoastlines_statistics as deacl_stats\n",
    "\n",
    "def to_vector(df,\n",
    "              fname='test.shp',\n",
    "              x='x',\n",
    "              y='y',\n",
    "              crs='EPSG:3577',\n",
    "              output_crs='EPSG:3577'):\n",
    "    \n",
    "    # Convert datetimes to strings\n",
    "    df = df.copy()\n",
    "    is_datetime = df.dtypes == 'datetime64[ns]'\n",
    "    df.loc[:, is_datetime] = df.loc[:, is_datetime].astype(str) \n",
    "    \n",
    "    # Export to file\n",
    "    gdf = gpd.GeoDataFrame(data=df.loc[:, df.dtypes != 'datetime64[ns]'],\n",
    "                     geometry=gpd.points_from_xy(x=df[x], y=df[y]),\n",
    "                     crs=crs).to_crs(output_crs).to_file(fname)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "def export_eval(df, output_name, output_crs='EPSG:3577'):\n",
    "    \n",
    "    from shapely.geometry import box, Point, LineString\n",
    "\n",
    "    # Extract geometries\n",
    "    val_points = gpd.points_from_xy(x=df.val_x, y=df.val_y)\n",
    "    deacl_points = gpd.points_from_xy(x=df.deacl_x, y=df.deacl_y)\n",
    "    df_profiles = df.groupby('id').first()\n",
    "    profile_lines = df_profiles.apply(\n",
    "        lambda x: LineString([(x.start_x, x.start_y), (x.end_x, x.end_y)]), axis=1)\n",
    "\n",
    "    # Export validation points\n",
    "    val_gdf = gpd.GeoDataFrame(data=df,\n",
    "                               geometry=val_points,\n",
    "                               crs=output_crs).to_crs('EPSG:4326')\n",
    "    val_gdf.to_file(f'figures/eval/{output_name}_val.geojson', \n",
    "                    driver='GeoJSON')\n",
    "\n",
    "    # Export DEACL points\n",
    "    deacl_gdf = gpd.GeoDataFrame(data=df,\n",
    "                                 geometry=deacl_points,\n",
    "                                 crs=output_crs).to_crs('EPSG:4326')\n",
    "    deacl_gdf.to_file(f'figures/eval/{output_name}_deacl.geojson', \n",
    "                      driver='GeoJSON')\n",
    "\n",
    "    # Export profiles\n",
    "    profile_gdf = gpd.GeoDataFrame(data=df_profiles,\n",
    "                                 geometry=profile_lines,\n",
    "                                 crs=output_crs).to_crs('EPSG:4326')\n",
    "    profile_gdf.to_file(f'figures/eval/{output_name}_profiles.geojson', \n",
    "                        driver='GeoJSON')\n",
    "\n",
    "\n",
    "def deacl_val_stats(val_dist, deacl_dist, n=None, remove_bias=False):\n",
    "\n",
    "    np.seterr(all='ignore')\n",
    "\n",
    "    # Compute difference and bias\n",
    "    diff_dist = val_dist - deacl_dist\n",
    "    bias = diff_dist.mean()\n",
    "    \n",
    "    if remove_bias:\n",
    "        deacl_dist += bias\n",
    "        diff_dist = val_dist - deacl_dist\n",
    "\n",
    "    # Compute stats\n",
    "    if n is None:\n",
    "        n = len(val_dist)\n",
    "    else:\n",
    "        n = sum(n)\n",
    "        \n",
    "    mae = mean_absolute_error(val_dist, deacl_dist)\n",
    "    rmse = mean_squared_error(val_dist, deacl_dist)**0.5\n",
    "    \n",
    "    \n",
    "    if n > 1:\n",
    "        corr = np.corrcoef(x=val_dist, y=deacl_dist)[0][1]\n",
    "        stdev = diff_dist.std()\n",
    "        \n",
    "    else:\n",
    "        corr = np.nan\n",
    "        stdev = np.nan\n",
    "\n",
    "    return pd.Series({\n",
    "        'n': n,\n",
    "        'mae': f'{mae:.2f}',\n",
    "        'rmse': f'{rmse:.2f}',\n",
    "        'stdev': f'{stdev:.2f}',\n",
    "        'corr': f'{corr:.3f}',\n",
    "        'bias': f'{bias:.2f}',\n",
    "    }).astype(float)\n",
    "\n",
    "\n",
    "def rse_tableformat(not_bias_corrected, bias_corrected, groupby='source'):\n",
    "\n",
    "    # Fix rounding and total observations\n",
    "    not_bias_corrected['n'] = not_bias_corrected['n'].astype(int)\n",
    "    not_bias_corrected[['bias', 'stdev', 'mae', 'rmse']] = not_bias_corrected[['bias', 'stdev', 'mae', 'rmse']].round(1)\n",
    "    not_bias_corrected['n'] = not_bias_corrected.groupby(groupby)['n'].sum()\n",
    "    \n",
    "    # Move bias corrected values into brackets\n",
    "    not_bias_corrected['MAE (m)'] = (not_bias_corrected.mae.astype('str') + ' (' + \n",
    "                                 bias_corrected.mae.round(1).astype('str') + ')')\n",
    "    not_bias_corrected['RMSE (m)'] = (not_bias_corrected.rmse.astype('str') + ' (' + \n",
    "                                  bias_corrected.rmse.round(1).astype('str') + ')')\n",
    "    \n",
    "    # Sort by MAE, rename columns\n",
    "    not_bias_corrected = (not_bias_corrected.sort_values('mae')\n",
    "     .drop(['mae', 'rmse'], axis=1)\n",
    "     .rename({'stdev': 'SD (m)', 'corr': 'Correlation', 'bias': 'Bias (m)'}, axis=1)\n",
    "             [['n', 'Bias (m)', 'MAE (m)', 'RMSE (m)', 'SD (m)', 'Correlation']])\n",
    "\n",
    "    return not_bias_corrected\n",
    "\n",
    "\n",
    "rename_dict = {\n",
    "    'Beachrock undiff': 'rocky',\n",
    "    'Beachrock undiff dominant': 'rocky',\n",
    "    'Boulder or shingle-grade beach undiff': 'rocky',\n",
    "    'Boulder groyne or breakwater undiff': 'rocky',\n",
    "    'Flat boulder deposit (rock) undiff': 'rocky',\n",
    "    'Hard bedrock shore': 'rocky',\n",
    "    'Hard bedrock shore inferred': 'rocky',\n",
    "    'Hard rock cliff (>5m)': 'rocky',\n",
    "    'Hard rocky shore platform': 'rocky',\n",
    "    'Rocky shore platform (undiff)': 'rocky',\n",
    "    'Sloping boulder deposit (rock) undiff': 'rocky',\n",
    "    'Sloping hard rock shore': 'rocky',\n",
    "    'Sloping soft `bedrock¿ shore': 'rocky',\n",
    "    'Sloping soft \\u2018bedrock\\u2019 shore': 'rocky',\n",
    "    'Soft `bedrock¿ shore inferred': 'rocky',\n",
    "    'Soft `bedrock¿ shore platform': 'rocky',\n",
    "    'Beach (sediment type undiff)': 'sandy',\n",
    "    'Fine-medium sand beach': 'sandy',\n",
    "    'Fine-medium sandy tidal flats': 'sandy',\n",
    "    'Mixed sand and shell beach': 'sandy',\n",
    "    'Mixed sandy shore undiff': 'sandy',\n",
    "    'Perched sandy beach (undiff)': 'sandy',\n",
    "    'Sandy beach undiff': 'sandy',\n",
    "    'Sandy beach with cobbles/pebbles (rock)': 'sandy',\n",
    "    'Sandy shore undiff': 'sandy',\n",
    "    'Sandy tidal flats': 'sandy',\n",
    "    'Sandy tidal flats with coarse stony debris': 'sandy',\n",
    "    'Sandy tidal flats, no bedrock protruding': 'sandy',\n",
    "    'Sloping coffee rock deposit': 'rocky',\n",
    "    'Muddy tidal flats': 'muddy',\n",
    "    'Tidal flats (sediment undiff)': 'muddy',\n",
    "    'Artificial shoreline undiff': 'rocky',\n",
    "    'Artificial boulder structures undiff': 'rocky',\n",
    "    'Boulder revetment': 'rocky',\n",
    "    'Boulder seawall': 'rocky',\n",
    "    'Concrete sea wall': 'rocky',\n",
    "    'Piles (Jetty)': 'rocky',\n",
    "    'Coarse sand beach': 'sandy'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_vector(output_stats, fname='test6.shp', x='0_x', y='0_y', crs='EPSG:3577')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunshine Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['8.Pumicestone - Bribie', '1.Coolum-Sunshine', '5.Dicky Beach', \n",
    "        '7.Kings Beach', '3.Mooloolaba', '2.Mudjimba-Yaroomba', '6.Shelly Beach',\n",
    "        '4.South Mooloolaba']\n",
    "\n",
    "for site in sites:\n",
    "    deacl_val.preprocess_sunshinecoast(site, datum=0, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moruya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_moruya(fname_out='output_data/moruya.csv', datum=0, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victoria/Deakin\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_vicdeakin(fname='input_data/vicdeakin/z_data_10cm_VIC.csv',\n",
    "                               datum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRL Narrabeen \n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_narrabeen(fname='input_data/wrl/Narrabeen_Profiles_2019.csv',\n",
    "                               datum=0,\n",
    "                               overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSW Beach Profile Database\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for fname in glob.glob('input_data/nswbpd/*.csv'):\n",
    "        pool.apply_async(deacl_val.preprocess_nswbpd, \n",
    "                         [fname, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "# fname = '/g/data/r78/DEACoastlines/validation/input_data/nswbpd/photogrammetry_Xsections_Lennox Head.csv'\n",
    "# profiles_df, intercept_df = deacl_val.preprocess_nswbpd(fname, 0, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City of Gold Coast\n",
    "* [X] Renovated\n",
    "* [ ] Fix North Kirra name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['BILINGA', 'BROADBEACH', 'BURLEIGH HEADS', 'COOLANGATTA', 'CURRUMBIN',\n",
    "         'DURANABH', 'FINGAL', 'GREENMOUNT HILL', 'KINGSCLIFF', 'KIRRA',\n",
    "         'MAIN BEACH', 'MERMAID BEACH', 'MIAMI', 'Main Beach Cross Sections',\n",
    "         'NARROWNECK', 'NO*TlH KIRRA', 'PALM BEACH', 'POINT DANGER', \n",
    "         'RAINBOW BAY', 'SEAWAY CENTRE LINE', 'SNAPPER ROCKS', \n",
    "         'SOUTH STRADBROKE', 'SURFERS PARADISE', 'THE SPIT', 'TUGUN', \n",
    "         'TWEED RIVER ENTRANCE']\n",
    "# sites=['MAIN BEACH']\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for site in sites:\n",
    "        pool.apply_async(deacl_val.preprocess_cgc, \n",
    "                         [site, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASMARC\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sites to iterate over\n",
    "sites = [i.split('/')[2] for i in glob.glob('input_data/tasmarc/*/')]\n",
    "# sites = sites[2:]\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for site in sites:\n",
    "        pool.apply_async(deacl_val.preprocess_tasmarc, \n",
    "                         [site, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WA DoT\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_gdf = gpd.read_file('input_data/WA_tertiaryCC.shp').to_crs('EPSG:3577').iloc[::-1]\n",
    "regions_gdf.index = (regions_gdf.LABEL\n",
    "                     .str.replace(' - ', '_')\n",
    "                     .str.replace('-', '')\n",
    "                     .str.replace(' ', '')\n",
    "                     .str.replace('/', '')\n",
    "                     .str.replace(',', '')\n",
    "                     .str.replace('_', '-')\n",
    "                     .str.lower())\n",
    "regions_gdf.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.coastal_transects_parallel(\n",
    "    regions_gdf,\n",
    "    interval=200,\n",
    "    transect_length=500,\n",
    "    simplify_length=200,\n",
    "    transect_buffer=50,\n",
    "    overwrite=False,\n",
    "    output_path='input_data/coastal_transects_wadot.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "    for i, _ in regions_gdf.iterrows():\n",
    "        pool.apply_async(deacl_val.preprocess_wadot, \n",
    "                         [regions_gdf.loc[[i]], False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DaSilva 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname='input_data/dasilva2021/dasilva_etal_2021_shorelines.shp'\n",
    "# val_gdf = gpd.read_file(fname).to_crs('EPSG:3577')\n",
    "# val_gdf = val_gdf.loc[val_gdf.Year_ > 1987]\n",
    "# val_gdf['Year_'] = val_gdf.Year_.astype(str)\n",
    "# val_gdf = val_gdf.set_index('Year_')\n",
    "\n",
    "\n",
    "# # transect_gdf = gpd.read_file('input_data/dasilva2021/dasilva_etal_2021_retransects.shp').to_crs('EPSG:3577')[['id', 'order', 'geometry']]\n",
    "# transect_gdf.columns = ['profile', 'order', 'geometry']\n",
    "\n",
    "\n",
    "\n",
    "# ransect_gdf['profile'] = transect_gdf.profile.astype(str)\n",
    "#     transect_gdf = gpd.clip(gdf=transect_gdf, mask=compartment, keep_geom_type=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_dasilva2021()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box, Point, LineString\n",
    "\n",
    "# Add measurement metadata\n",
    "intersect_gdf[['start_x', 'start_y']] = intersect_gdf.apply(\n",
    "    lambda x: pd.Series(x.geometry.coords[0]), axis=1)\n",
    "intersect_gdf[['end_x', 'end_y']] = intersect_gdf.apply(\n",
    "    lambda x: pd.Series(x.geometry.coords[1]), axis=1)\n",
    "intersect_gdf['0_dist'] = intersect_gdf.apply(\n",
    "    lambda x: Point(x.start_x, x.start_y).distance(x['val_point']), axis=1)\n",
    "intersect_gdf[['0_x', '0_y']] = intersect_gdf.apply(\n",
    "    lambda x: pd.Series(x.val_point.coords[0][0:2]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WA DoT - Stirling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_stirling(fname_out='output_data/stirling_stirling.csv',\n",
    "                              datum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SA Department of Environment and Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['200048',\n",
    " '320010',\n",
    " '320011',\n",
    " '330005',\n",
    " '330014',\n",
    " '425001',\n",
    " '425002',\n",
    " '440004',\n",
    " '525019',\n",
    " '525022',\n",
    " '525023',\n",
    " '530009',\n",
    " '545001',\n",
    " '555007',\n",
    " '555012',\n",
    " '815013']\n",
    "\n",
    "fname = f'input_data/sadew/{sites[15]}.CSV'\n",
    "print(fname)\n",
    "profile_df = deacl_val.preprocess_sadew(fname, datum=0, overwrite=True)\n",
    "profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# for fname in fname_list:\n",
    "#     preprocess_sadew(fname, datum=0, overwrite=False)\n",
    "\n",
    "fname_list = glob.glob('input_data/sadew/*.CSV')    \n",
    "\n",
    "with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "    for fname in fname_list:\n",
    "        pool.apply_async(deacl_val.preprocess_sadew, \n",
    "                         [fname, 0, True])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fellowes et al. 20221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "from pyproj import Transformer\n",
    "from itertools import takewhile\n",
    "from scipy import stats\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from shapely.geometry import box, Point, LineString\n",
    "pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx', sheet_name=0, header=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx',\n",
    "                       sheet_name='Profile Locations',\n",
    "                       header=1,\n",
    "                       names=['estuary', 'beach', 'profile', 'start_y', 'start_x', 'end_y', 'end_x' ]).drop('estuary', axis=1)\n",
    "coords['beach'] = coords.beach.str.replace(\" \", \"\").str.replace(\"(\", \"\").str.replace(\")\", \"\").str.replace('Fishermans','Frenchmans').str.lower()\n",
    "coords['section'] = 'all'\n",
    "coords['name'] = 'fellowes2021'\n",
    "coords['source'] = 'aerial photogrammetry'\n",
    "coords['slope'] = np.nan\n",
    "coords['id'] = (coords.beach + '_' + coords.section + '_' + coords.profile)\n",
    "\n",
    "# Reproject coords to Albers and create geodataframe\n",
    "trans = Transformer.from_crs('EPSG:4326', 'EPSG:3577', always_xy=True)\n",
    "coords['start_x'], coords['start_y'] = trans.transform(coords.start_x.values,\n",
    "                                                       coords.start_y.values)\n",
    "coords['end_x'], coords['end_y'] = trans.transform(coords.end_x.values,\n",
    "                                                   coords.end_y.values)\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowes_data = pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx', None)\n",
    "beach_list = list(fellowes_data.keys())[1:]\n",
    "\n",
    "for name in beach_list:\n",
    "\n",
    "    beach = name.split(' - ')[1].replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "    fname_out = f'output_data/fellowes2021_{beach}.csv'\n",
    "    print(f'Processing {beach:<80}', end='\\r')\n",
    "\n",
    "    # Load data and convert to long format\n",
    "    wide_df = fellowes_data[name]\n",
    "    profiles_df = pd.melt(wide_df, \n",
    "                          id_vars='Date', \n",
    "                          var_name='profile', \n",
    "                          value_name='0_dist').rename({'Date': 'date'}, axis=1)\n",
    "    profiles_df['date'] = pd.to_datetime(profiles_df.date, yearfirst=True)\n",
    "    profiles_df['id'] = (f'{beach}_all_' + profiles_df.profile)\n",
    "\n",
    "    # Remove negative distances\n",
    "    profiles_df = profiles_df.loc[profiles_df['0_dist'] >= 0]\n",
    "\n",
    "    # Restrict to post 1987\n",
    "    profiles_df = profiles_df[(profiles_df.date.dt.year > 1987)]\n",
    "\n",
    "    # Merge profile coordinate data into transect data\n",
    "    profiles_df = profiles_df.merge(coords, on=['id', 'profile'])\n",
    "\n",
    "    # Add coordinates at supplied distance along transects\n",
    "    profiles_df[['0_x', '0_y']] = profiles_df.apply(\n",
    "        lambda x: pd.Series(deacl_val.dist_along_transect(x['0_dist'], \n",
    "                                                x.start_x, \n",
    "                                                x.start_y,\n",
    "                                                x.end_x,\n",
    "                                                x.end_y)), axis=1)\n",
    "\n",
    "    # Keep required columns\n",
    "    shoreline_df = profiles_df[['id', 'date', 'beach', \n",
    "                                'section', 'profile', 'name',\n",
    "                                'source', 'slope', 'start_x', \n",
    "                                'start_y', 'end_x', 'end_y', \n",
    "                                '0_dist', '0_x', '0_y']]\n",
    "\n",
    "    shoreline_df.to_csv(fname_out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "To investigate:\n",
    "* [ ] **Cluster of outliers at Evans Head and Broadwater**\n",
    "* [ ] **Extreme outliers at Flynn's Beach**\n",
    "* [ ] **Skewed result at Gerringong Harbour**\n",
    "* [ ] Outliers in Hargrave's Beach\n",
    "* [ ] Remaining outliers at Jimmy's Beach\n",
    "* [ ] Outliers at Shellharbour Little Lake\n",
    "* [ ] **Several remaining outliers at Swansea Belmont**\n",
    "* [ ] **Major issues with Valla Beach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_data/tasmarc_east_cove.csv                                               output_data/tasmarc_alonnah.csv                                                 output_data/tasmarc_nine_mile_beach_south.csv                                   output_data/tasmarc_cockle_creek.csv                                            output_data/tasmarc_primrose_sands.csv                                          output_data/tasmarc_seven_mile_beach.csv                                        output_data/tasmarc_coles_beach.csv                                             \r"
     ]
    }
   ],
   "source": [
    "import random \n",
    "val_paths = glob.glob('output_data/tasmarc*.csv')\n",
    "random.shuffle(val_paths)\n",
    "# deacl_path = '/g/data/r78/DEACoastlines/releases/DEACoastlines_v1.0.0/Shapefile/DEACoastlines_annualcoastlines_v1.0.0.shp'\n",
    "deacl_path = '/g/data/r78/rt1527/dea-notebooks/Testing/ARD_comparison/ga_coastlines_tasmania.geojson'\n",
    "\n",
    "prefix='ard_comparison_ga'\n",
    "\n",
    "# Parallelised\n",
    "with mp.Pool(6) as pool:\n",
    "    for val_path in val_paths:\n",
    "        \n",
    "        # Run analysis and close resulting figure\n",
    "        pool.apply_async(deacl_val.deacl_validation, \n",
    "                         [val_path, deacl_path, 0, prefix, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "# # Non-parallel\n",
    "# for val_path in val_paths:\n",
    "#     try:\n",
    "#         deacl_val.deacl_validation(val_path, deacl_path, 0, prefix, True)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "val_paths = glob.glob('output_data/tasmarc_*.csv')\n",
    "random.shuffle(val_paths)\n",
    "# deacl_path = '/g/data/r78/DEACoastlines/releases/DEACoastlines_v1.0.0/Shapefile/DEACoastlines_annualcoastlines_v1.0.0.shp'\n",
    "deacl_path = '/g/data/r78/rt1527/dea-notebooks/Testing/ARD_comparison/usgs_coastlines_tasmania.geojson'\n",
    "\n",
    "prefix='ard_comparison_usgs'\n",
    "\n",
    "# Parallelised\n",
    "with mp.Pool(6) as pool:\n",
    "    for val_path in val_paths:\n",
    "        \n",
    "        # Run analysis and close resulting figure\n",
    "        pool.apply_async(deacl_val.deacl_validation, \n",
    "                         [val_path, deacl_path, 0, prefix, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "# # Non-parallel\n",
    "# for val_path in val_paths:\n",
    "#     try:\n",
    "#         deacl_val.deacl_validation(val_path, deacl_path, 0, prefix, True)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n        2067.000\n",
       "mae        12.720\n",
       "rmse       16.430\n",
       "stdev      13.040\n",
       "corr        0.985\n",
       "bias       10.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load all results into a single file\n",
    "# print('Combining data')\n",
    "# stats_list = glob.glob(f'{prefix}_*.csv')\n",
    "# stats_df = pd.concat([pd.read_csv(csv) for csv in stats_list])  #.query(\"beach == 'narrabeen'\") #.query(\"n >= 25\")\n",
    "\n",
    "# # Rename smartline categories to smaller subset\n",
    "# stats_df['smartline'] = stats_df.smartline.replace(rename_dict)\n",
    "\n",
    "# # Export to file\n",
    "# # stats_df.to_csv('deacl_all_results.csv', index=False)\n",
    "\n",
    "# # Run stats\n",
    "# deacl_val_stats(stats_df.val_dist, stats_df.deacl_dist, n=stats_df.n, remove_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation vector\n",
    "output_name = prefix\n",
    "export_eval(stats_df.set_index('id'), output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "nswbpd    329\n",
       "wadot     482\n",
       "wrl         5\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.groupby('name')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats_df.id.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARD comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Bias (m)</th>\n",
       "      <th>MAE (m)</th>\n",
       "      <th>RMSE (m)</th>\n",
       "      <th>SD (m)</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smartline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rocky</th>\n",
       "      <td>28</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>5.4 (5.0)</td>\n",
       "      <td>7.5 (7.3)</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandy</th>\n",
       "      <td>2036</td>\n",
       "      <td>10.4</td>\n",
       "      <td>13.0 (10.0)</td>\n",
       "      <td>16.6 (13.0)</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              n  Bias (m)      MAE (m)     RMSE (m)  SD (m)  Correlation\n",
       "smartline                                                               \n",
       "rocky        28      -1.9    5.4 (5.0)    7.5 (7.3)     7.4        0.993\n",
       "sandy      2036      10.4  13.0 (10.0)  16.6 (13.0)    13.0        0.986"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all results into a single file\n",
    "prefix='ard_comparison_ga'\n",
    "stats_list = glob.glob(f'{prefix}_*.csv')\n",
    "stats_df = pd.concat([pd.read_csv(csv) for csv in stats_list])  #.query(\"beach == 'narrabeen'\") #.query(\"n >= 25\")\n",
    "\n",
    "# Rename smartline categories to smaller subset\n",
    "stats_df['smartline'] = stats_df.smartline.replace(rename_dict)\n",
    "\n",
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "ga_df = rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "ga_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Bias (m)</th>\n",
       "      <th>MAE (m)</th>\n",
       "      <th>RMSE (m)</th>\n",
       "      <th>SD (m)</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smartline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rocky</th>\n",
       "      <td>27</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>7.1 (7.0)</td>\n",
       "      <td>11.5 (10.9)</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandy</th>\n",
       "      <td>2033</td>\n",
       "      <td>9.4</td>\n",
       "      <td>12.8 (10.4)</td>\n",
       "      <td>16.4 (13.4)</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              n  Bias (m)      MAE (m)     RMSE (m)  SD (m)  Correlation\n",
       "smartline                                                               \n",
       "rocky        27      -3.6    7.1 (7.0)  11.5 (10.9)    11.1        0.982\n",
       "sandy      2033       9.4  12.8 (10.4)  16.4 (13.4)    13.4        0.985"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all results into a single file\n",
    "prefix='ard_comparison_usgs'\n",
    "stats_list = glob.glob(f'{prefix}_*.csv')\n",
    "stats_df = pd.concat([pd.read_csv(csv) for csv in stats_list])  #.query(\"beach == 'narrabeen'\") #.query(\"n >= 25\")\n",
    "\n",
    "# Rename smartline categories to smaller subset\n",
    "stats_df['smartline'] = stats_df.smartline.replace(rename_dict)\n",
    "\n",
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "usgs_df = rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "usgs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results by substrate\n",
    "\n",
    "##### Greater than Landsat frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Bias (m)</th>\n",
       "      <th>MAE (m)</th>\n",
       "      <th>RMSE (m)</th>\n",
       "      <th>SD (m)</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smartline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sandy</th>\n",
       "      <td>1064</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.6 (4.1)</td>\n",
       "      <td>6.9 (5.9)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              n  Bias (m)    MAE (m)   RMSE (m)  SD (m)  Correlation\n",
       "smartline                                                           \n",
       "sandy      1064       3.6  5.6 (4.1)  6.9 (5.9)     6.0        0.986"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.query(\"n >= 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.query(\"n >= 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "out = rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Less than Landsat frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.query(\"n < 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.query(\"n < 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "out = rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation by rate of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'slope_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b19be5d8d196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     suffixes=('_val', '_deacl'))\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdeacl_val_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslope_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslope_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacl_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslope_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslope_deacl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20200713/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'slope_val'"
     ]
    }
   ],
   "source": [
    "long_temporal = stats_df.groupby(['id']).filter(lambda x: len(x) >= 10).set_index('id')\n",
    "# test = stats_df.groupby(['id']).filter(lambda x: (np.ptp(x.year) > 25) & (len(x) >= 5)).set_index('id')\n",
    "\n",
    "# Compute rates of change for both validation and DEAC data\n",
    "deacl_rates = long_temporal.groupby(['id']).apply(lambda x: deacl_stats.change_regress(\n",
    "    y_vals=x.deacl_dist, x_vals=x.year, x_labels=x.year))\n",
    "val_rates = long_temporal.groupby(['id']).apply(lambda x: deacl_stats.change_regress(\n",
    "    y_vals=x.val_dist, x_vals=x.year, x_labels=x.year))\n",
    "\n",
    "# Combine rates of change\n",
    "slope_df = pd.merge(val_rates, \n",
    "                    deacl_rates, \n",
    "                    left_index=True, \n",
    "                    right_index=True, \n",
    "                    suffixes=('_val', '_deacl'))\n",
    "\n",
    "deacl_val_stats(val_dist=slope_df.slope_val, deacl_dist=slope_df.slope_deacl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val_stats(val_dist=slope_df.slope_val, deacl_dist=slope_df.slope_deacl, remove_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df_sig = slope_df.loc[(slope_df.pvalue_deacl <= 0.01) | (slope_df.pvalue_val <= 0.01)]\n",
    "deacl_val_stats(val_dist=slope_df_sig.slope_val, deacl_dist=slope_df_sig.slope_deacl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val_stats(val_dist=slope_df_sig.slope_val, deacl_dist=slope_df_sig.slope_deacl, remove_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.set_index('id').loc[slope_df.index].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.set_index('id').loc[slope_df_sig.index].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all results into a single file\n",
    "# print('Combining data')\n",
    "# stats_list = glob.glob(f'{prefix}_*.csv')\n",
    "# stats_df = pd.concat([pd.read_csv(csv) for csv in stats_list])\n",
    "# # stats_df.to_csv('deacl_all_results.csv', index=False)\n",
    "\n",
    "# deacl_val_stats(stats_df.val_dist, stats_df.deacl_dist, n=stats_df.n, remove_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deacl_val_stats(stats_df.val_dist, stats_df.deacl_dist, n=stats_df.n, remove_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = stats_df.loc[stats_df.id == 'ladyrobinsons_all_G6']\n",
    "# test.plot(x='year', y='deacl_dist')\n",
    "# test.plot(x='year', y='val_dist', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_df.set_index('id').loc[slope_df.index.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "stats_df = pd.read_csv('deacl_all_results.csv')\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stats_df.smartline.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset stats\n",
    "\n",
    "Plot of validation sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_source = stats_df.groupby('name').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_source_nobias = stats_df.groupby('name').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "rse_tableformat(by_source, by_source_nobias, 'name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_gdf = gpd.GeoDataFrame(data=stats_df,\n",
    "                             geometry=gpd.points_from_xy(\n",
    "                                 x=stats_df.lon,\n",
    "                                 y=stats_df.lat,\n",
    "                                 crs='EPSG:4326')).to_crs('EPSG:3577')\n",
    "\n",
    "stats_gdf.to_file('../bishoptaylor_2020/Validation_extent/validation_points.shp')\n",
    "\n",
    "# Load and reverse buffer Australian boundary\n",
    "aus_inside = (gpd.read_file('/g/data/r78/rt1527/shapefiles/australia/australia/cstauscd_r.shp')\n",
    " .query(\"FEAT_CODE=='mainland'\")\n",
    " .to_crs('EPSG:3577')\n",
    " .assign(dissolve=1)\n",
    " .dissolve('dissolve')\n",
    " .simplify(10000)\n",
    " .buffer(-100000)\n",
    " .buffer(50000))\n",
    "\n",
    "# Compute convex hulls for each validation dataset\n",
    "convex_hulls = stats_gdf.dissolve('name').convex_hull.buffer(50000)\n",
    "\n",
    "# Clip convex hulls by Australia coastline\n",
    "gpd.overlay(gpd.GeoDataFrame(geometry=convex_hulls), \n",
    "            gpd.GeoDataFrame(geometry=aus_inside), \n",
    "            how='difference').buffer(100000).to_file('../bishoptaylor_2020/Validation_extent/validation_extent.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of validation sites by source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")['year'].agg([np.min,np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = dict(zip(stats_df.groupby([\"source\"])['year'].count().sort_values().index[0:4], ['Other']*4))\n",
    "rename_dict = {**rename_dict, **{'aerial photogrammetry': 'Aerial photogrammetry',\n",
    "                                 'drone photogrammetry': 'Drone photogrammetry',\n",
    "                                 'hydrographic survey': 'Hydrographic survey',\n",
    "                                 'lidar': 'LiDAR'}}\n",
    "\n",
    "counts_per_year = (stats_df\n",
    "                   .pipe(lambda x: x.assign(source_updated = x.source.replace(rename_dict)))\n",
    "                   .groupby([\"year\", \"source_updated\"])['n']\n",
    "                   .sum()\n",
    "                   .unstack()) \n",
    "counts_per_year = counts_per_year / counts_per_year.sum().sum()\n",
    "counts_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(11, 8))\n",
    "counts_per_year.plot(ax=ax, \n",
    "                     kind='bar', \n",
    "                     stacked=True, \n",
    "                     width=1.0, \n",
    "                     edgecolor='#484746', \n",
    "                     linewidth=1.0, \n",
    "                     color=['#e5c494', '#ff7f0e', '#7295c1', '#8dbe8d', 'lightgrey']\n",
    "                    ) \n",
    "ax.yaxis.set_ticks_position('right')\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.xaxis.label.set_visible(False)\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20], ['0%', '5%', '10%', '15%', '20%']);\n",
    "plt.legend(loc=\"upper left\", ncol=5, bbox_to_anchor=(0, -0.08), fancybox=False, shadow=False, frameon=False)\n",
    "\n",
    "# Export to file\n",
    "fig.savefig(fname='../bishoptaylor_2020/Validation_extent/validation_temporal2.png', \n",
    "            dpi=300, pad_inches=0, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")[\"source\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['n'] / out['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby('name')['smartline'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results by slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "by_slope_nobias.plot.scatter(x='slope', y='mae', c='C1', ax=ax1, s=6)\n",
    "by_slope.plot.scatter(x='slope', y='mae', c='C0', ax=ax1, s=6)\n",
    "by_yearly_obs_nobias.plot.scatter(x='n', y='mae', c='C1', ax=ax2, s=6)\n",
    "by_yearly_obs.plot.scatter(x='n', y='mae', c='C0', ax=ax2, s=6)\n",
    "by_slope.plot.scatter(x='slope', y='bias', c='dimgrey', ax=ax1, s=6)\n",
    "by_yearly_obs.plot.scatter(x='n', y='bias', c='dimgrey', ax=ax2, s=6)\n",
    "\n",
    "# Add trendline and restrict extent\n",
    "z = lowess(by_slope['mae'], by_slope['slope'])\n",
    "ax1.plot(z[:, 0], z[:, 1], '--', color = 'C0', linewidth = 1.3, zorder=3);\n",
    "z = lowess(by_slope_nobias['mae'], by_slope_nobias['slope'])\n",
    "ax1.plot(z[:, 0], z[:, 1], '--', color = 'C1', linewidth = 1.3);\n",
    "z = lowess(by_yearly_obs['mae'], by_yearly_obs['n'])\n",
    "ax2.plot(z[:, 0], z[:, 1], '--', color = 'C0', linewidth = 1.3, zorder=3);\n",
    "z = lowess(by_yearly_obs_nobias['mae'], by_yearly_obs_nobias['n'])\n",
    "ax2.plot(z[:, 0], z[:, 1], '--', color = 'C1', linewidth = 1.3);\n",
    "z = lowess(by_slope['bias'], by_slope['slope'])\n",
    "ax1.plot(z[:, 0], z[:, 1], '--', color = 'dimgrey', linewidth = 1.0, zorder=0);\n",
    "z = lowess(by_yearly_obs['bias'], by_yearly_obs['n'])\n",
    "ax2.plot(z[:, 0], z[:, 1], '--', color = 'dimgrey', linewidth = 1.0, zorder=0);\n",
    "\n",
    "# Set axis limita\n",
    "ax1.set_xticks(np.arange(0, 0.2, 0.02))\n",
    "ax1.set_ylabel('Metres (m)')\n",
    "ax1.set_xlabel('Slope')\n",
    "ax2.set_xlabel('Survey frequency (observations per year)')\n",
    "ax2.set_ylim(0, 25)\n",
    "ax2.yaxis.label.set_visible(False)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "ax1.legend(['Mean Absolute Error (MAE)', 'Bias-corrected MAE', 'Bias'], frameon=False)\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax1.spines[axis].set_linewidth(1.5)\n",
    "    ax2.spines[axis].set_linewidth(1.5)\n",
    "\n",
    "# plt.savefig(fname=f'../bishoptaylor_2020/SlopeObs/FigureX_Effectofslopeandobs.png', \n",
    "#             bbox_inches='tight',\n",
    "#             transparent=True,\n",
    "#             pad_inches=0.05, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "by_yearly_obs = stats_df.groupby('n').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, False)).drop('n', axis=1).reset_index()\n",
    "by_yearly_obs.plot.scatter(x='n', y='mae', ax=ax2, s=15)\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax1.spines[axis].set_linewidth(1.5)\n",
    "    ax2.spines[axis].set_linewidth(1.5)\n",
    "\n",
    "by_slope = stats_df[['slope', 'error_m']].dropna(axis=0) \n",
    "by_slope['slope'] = by_slope.slope.abs()\n",
    "\n",
    "sns.kdeplot(ax=ax1,\n",
    "            data=by_slope['slope'],\n",
    "            data2=by_slope['error_m'],\n",
    "            cmap='YlOrRd',\n",
    "            legend=True, \n",
    "            cbar=True,\n",
    "            shade=True,\n",
    "            shade_lowest=False,\n",
    "            levels=16,\n",
    "            clip=([0, 0.202], [-20, 55]),\n",
    "            cbar_kws={\"use_gridspec\":False, \n",
    "                      \"location\":\"top\",\n",
    "                      \"shrink\":0.5, \n",
    "                      \"anchor\":(0.92, 0.0),\n",
    "                      'label': 'Density',\n",
    "                      'ticklocation':\"left\",\n",
    "                      \"ticks\": [0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n",
    "           )\n",
    "\n",
    "# sns.kdeplot(ax=ax1,\n",
    "#             data=by_slope['slope'],\n",
    "#             data2=by_slope['error_m'],\n",
    "#             colors=\"black\",\n",
    "#             shade=False,\n",
    "#             shade_lowest=False,\n",
    "#             levels=16,\n",
    "#             clip=([0, 0.202], [-20, 55]),\n",
    "#             linewidths=0.5\n",
    "# #             linestyles=\":\"\n",
    "#            )\n",
    "\n",
    "# Add trendline and restrict extent\n",
    "z = lowess(by_yearly_obs['mae'], by_yearly_obs['n'])\n",
    "ax2.plot(z[:, 0], z[:, 1], '--', color = 'black', linewidth = 1.3, zorder=3);\n",
    "\n",
    "ax1.set(ylim=(-25, 55))\n",
    "# # Set axis limita\n",
    "# ax1.set_xticks(np.arange(0, 0.2, 0.02))\n",
    "ax1.set_ylabel('Errors (m)')\n",
    "ax2.set_ylabel('Mean Absolute Error (MAE)')\n",
    "ax1.set_xlabel('Intertidal slope (tan θ)')\n",
    "ax2.set_xlabel('Survey frequency (observations per year)')\n",
    "ax2.set_ylim(0, 25)\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0)\n",
    "ax2.legend(['Trendline (LOESS)'], frameon=False)\n",
    "ax1.axhline(y=0, linestyle='--', color='black', linewidth = 0.5)\n",
    "\n",
    "\n",
    "ax1.annotate('Landward\\nbias', \n",
    "             xytext=(0.1817, 2), \n",
    "             xy=(0.1817, 13.8), \n",
    "             arrowprops={'arrowstyle': '-|>', \n",
    "                         'facecolor': 'black', \n",
    "                         'mutation_scale': 15}, \n",
    "             ha='center')\n",
    "ax1.annotate('Seaward\\nbias', \n",
    "             xytext=(0.1817, -7.5), \n",
    "             xy=(0.1817, -14), \n",
    "             arrowprops={'arrowstyle': '-|>', \n",
    "                         'facecolor': 'black', \n",
    "                         'mutation_scale': 15}, \n",
    "             ha='center')\n",
    "\n",
    "# plt.savefig(fname=f'../bishoptaylor_2020/SlopeObs/FigureX_Effectofslopeandobs.png', \n",
    "#             bbox_inches='tight',\n",
    "#             transparent=True,\n",
    "#             pad_inches=0.05, dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_var = 'id'\n",
    "\n",
    "ids_to_keep = ([f'dasilva2021_W_{i}' for i in range(1, 20, 2)] + \n",
    "               [f'dasilva2021_W_{i}' for i in range(20, 79, 5)] +\n",
    "               [f'dasilva2021_E_{i}' for i in range(1, 20, 2)] +\n",
    "               [f'dasilva2021_E_{i}' for i in range(20, 65, 5)])\n",
    "# ids_to_keep = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to specific IDs\n",
    "if ids_to_keep:\n",
    "    stats_subset = stats_df.loc[stats_df[id_var].isin(ids_to_keep)]\n",
    "    stats_subset = pd.concat([stats_subset.query(\"section == 'W'\").sort_values('profile', ascending=False),\n",
    "           stats_subset.query(\"section == 'E'\").sort_values('profile', ascending=True)])\n",
    "else:\n",
    "    stats_subset = stats_df\n",
    "    \n",
    "# Filter to frequency\n",
    "high_freq = stats_subset.groupby(id_var).filter(lambda x: len(x) >= 1)\n",
    "high_freq_stats = high_freq.groupby(id_var).apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "\n",
    "# Merge data and stats\n",
    "high_freq_merged = pd.merge(\n",
    "    left=high_freq,\n",
    "    right=high_freq_stats,\n",
    "    how=\"inner\",\n",
    "    on=id_var)\n",
    "\n",
    "# Optionally, remove bias\n",
    "high_freq_merged['deacl_dist'] += high_freq_merged['bias']\n",
    "\n",
    "# Melt dataframe to long\n",
    "high_freq_melted = pd.melt(high_freq_merged, #.query('corr > 0.8'), \n",
    "                    id_vars=[id_var, 'year'], \n",
    "                    value_vars=['val_dist', 'deacl_dist'], \n",
    "                    value_name='distance')\n",
    "\n",
    "# Optionally, standardise distance to min = 0\n",
    "# high_freq_melted['distance'] = high_freq_melted.distance - high_freq_melted.groupby(id_var).distance.transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lines on two facets\n",
    "g = sns.relplot(\n",
    "    data=high_freq_melted,\n",
    "    x=\"year\", y=\"distance\", hue='variable', kind=\"line\",\n",
    "    col=id_var, col_wrap=5,\n",
    "    aspect=1.6, height=1.3,\n",
    "#     facet_kws={'sharey': False, 'sharex': True},\n",
    "#     facet_kws={'ylim': (0, 75)\n",
    "              \n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.fig.savefig(f\"dasilva2021_comparison.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results by profile\n",
    "ID filtered to multi-year observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_id = stats_df.groupby('id').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_id.query(\"n > 5\").astype('float').boxplot(column='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_id.query(\"n > 5\")['mae'].astype('float').plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of validation source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_source = stats_df.groupby('source').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_source_nobias = stats_df.groupby('source').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "rse_tableformat(by_source, by_source_nobias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of yearly validation observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n > 1].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (stats_df[stats_df.n == 1].n.sum() / stats_df.n.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n >= 12].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n >= 12].n.sum()  / stats_df.n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n >= 12].beach.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_yearly_obs = stats_df.groupby('n').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, False)).drop('n', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "by_yearly_obs = stats_df.groupby('n').apply(\n",
    "    lambda x: deacl_val_stats(x.val_dist, x.deacl_dist, False)).drop('n', axis=1).reset_index()\n",
    "by_yearly_obs.plot.scatter(x='n', y='mae')\n",
    "\n",
    "# Add trendline and restrict extent\n",
    "z = lowess(by_yearly_obs['mae'], by_yearly_obs['n'])\n",
    "plt.gca().plot(z[:, 0], z[:, 1], '--', color = 'black', linewidth = 1, );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_slope = stats_df[['n', 'error_m']].dropna(axis=0) \n",
    "by_slope['error_m'] = by_slope.error_m.abs()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "a = sns.kdeplot(ax=plt.gca(),\n",
    "            data=by_slope['n'],\n",
    "            data2=by_slope['error_m'],\n",
    "            cmap='YlOrRd',\n",
    "            shade=True,\n",
    "            shade_lowest=False,\n",
    "#             levels=15,\n",
    "#             clip=([0, 0.18], [-20, 55]),\n",
    "           )\n",
    "\n",
    "# # Add trendline and restrict extent\n",
    "# z = lowess(by_slope['error_m'], by_slope['slope'])\n",
    "# plt.gca().plot(z[:, 0], z[:, 1], '--', color = 'black', linewidth = 1, )\n",
    "# a.set(xlim=(0, 0.18))\n",
    "# a.set(ylim=(-20, 55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_slope.plot.scatter(x='n', y='error_m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export evaluation vector\n",
    "output_name = 'vicdeakin'\n",
    "export_eval(stats_df, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.section.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats and plot scatterplot\n",
    "stats_subset = stats_df  #.query(\"section == '10' | section == '9c' | section == '9b' | section == '9a' | section == '8'\")  #.query(\"deacl_dist < 3000\")  #.loc[stats_df.id.isin(['bussleton_all_31'])]\n",
    "\n",
    "def val_plot(df, \n",
    "             title='Validation',\n",
    "             scatter=True, \n",
    "             density=True,\n",
    "             time=True, \n",
    "             time_stat='mean',\n",
    "             time_legend_pos=[0.8, 0.035],\n",
    "             offset=0,\n",
    "             extent=(0, 120),\n",
    "             scatter_kwargs={}, \n",
    "             time_kwargs={}):\n",
    "    \n",
    "    # Copy data and apply offset\n",
    "    df = df.copy()\n",
    "    df['error_m'] += offset\n",
    "    df['deacl_dist'] += offset\n",
    "\n",
    "    # Compute stats  \n",
    "    n, mae, rmse, stdev, corr, bias = deacl_val_stats(\n",
    "        val_dist=df.val_dist, \n",
    "        deacl_dist=df.deacl_dist)    \n",
    "    offset_str = 'landward offset' if bias > 0 else 'ocean-ward offset'\n",
    "    print(bias)\n",
    "\n",
    "    if scatter:\n",
    "        \n",
    "        # Plot data as scatterplot\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        df.plot.scatter(x='val_dist',\n",
    "                        y='deacl_dist',\n",
    "                        s=15,\n",
    "                        edgecolors='black',\n",
    "                        linewidth=0,\n",
    "#                         xlim=extent,\n",
    "#                         ylim=extent,\n",
    "                        ax=ax,\n",
    "                        **scatter_kwargs)\n",
    "        \n",
    "        # Add dashed line\n",
    "        \n",
    "        \n",
    "        ax.plot(\n",
    "                np.linspace(df.loc[:, ['deacl_dist', 'val_dist']].values.min(), \n",
    "                            df.loc[:, ['deacl_dist', 'val_dist']].values.max()),\n",
    "                np.linspace(df.loc[:, ['deacl_dist', 'val_dist']].values.min(), \n",
    "                            df.loc[:, ['deacl_dist', 'val_dist']].values.max()),\n",
    "                color='black',\n",
    "                linestyle='dashed')\n",
    "        \n",
    "        ax.set_xlabel(f'{title} (metres along profile)')\n",
    "        ax.set_ylabel(f'DEA Coastlines (metres along profile)')\n",
    "        \n",
    "        # Add annotation\n",
    "        ax.annotate(f'Mean Absolute Error: {mae:.1f} m\\n' \\\n",
    "                    f'RMSE: {rmse:.1f} m\\n' \\\n",
    "                    f'Standard deviation: {stdev:.1f} m\\n' \\\n",
    "                    f'Bias: {bias:.1f} m {offset_str}\\n' \\\n",
    "                    f'Correlation: {corr:.2f}\\n',\n",
    "                    xy=(0.04, 0.75),\n",
    "                    fontsize=14,\n",
    "                    xycoords='axes fraction')\n",
    "\n",
    "        # Set title\n",
    "        plt.gca().set_title(f'DEA Coastlines vs {title}', weight='bold')\n",
    "        \n",
    "        # Export to file        \n",
    "        fig.savefig(f\"{title}_scatter_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    if density:\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "        g = sns.kdeplot(ax=ax,\n",
    "                        data=df.val_dist,\n",
    "                        data2=df.deacl_dist, \n",
    "                        cmap='YlOrRd', \n",
    "                        shade=True,\n",
    "                        bw=3,\n",
    "                        shade_lowest=False,\n",
    "                        clip=(extent, extent))\n",
    "\n",
    "        g.set(xlabel=f'{title} (metres along profile)', \n",
    "              ylabel=f'DEA Coastlines (metres along profile)')\n",
    "        ax.set_title(f'DEA Coastlines vs {title}', weight='bold')\n",
    "\n",
    "        # Add stats annotation\n",
    "        ax.annotate(f'Mean Absolute Error: {mae:.1f} m\\n' \\\n",
    "                    f'RMSE: {rmse:.1f} m\\n' \\\n",
    "                    f'Standard deviation: {stdev:.1f} m\\n' \\\n",
    "                    f'Bias: {bias:.1f} m {offset_str}\\n' \\\n",
    "                    f'Correlation: {corr:.2f}\\n',\n",
    "                    xy=(0.04, 0.75),\n",
    "                    fontsize=14,\n",
    "                    xycoords='axes fraction')\n",
    "\n",
    "        # Add diagonal line\n",
    "        plt.gca().plot(np.linspace(*extent), \n",
    "                       np.linspace(*extent),\n",
    "                       color='black',\n",
    "                       linestyle='dashed')\n",
    "        \n",
    "        plt.gca().set_ylim(bottom=extent[0])\n",
    "        plt.gca().set_xlim(left=extent[0])\n",
    "\n",
    "        # Export to file\n",
    "        fig = g.get_figure()\n",
    "        fig.savefig(f\"{title}_heatmap_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    if time:\n",
    "        \n",
    "        # Group by beach and apply statistic\n",
    "        stats_grouped = (df.groupby(['beach', 'year'], as_index=False)\n",
    "                         .aggregate(time_stat)\n",
    "                         .rename({'beach': 'id',\n",
    "                                  'deacl_dist': 'DEA Coastlines',\n",
    "                                  'val_dist': title}, axis=1)\n",
    "                         .groupby('id')\n",
    "                         .filter(lambda x: len(x) > 1))\n",
    "\n",
    "        # Melt data into long format for faceted plotting\n",
    "        stats_melted = pd.melt(stats_grouped, \n",
    "                               id_vars=['id', 'year'],\n",
    "                               value_vars=['DEA Coastlines', title],\n",
    "                               value_name='Distance (m)')\n",
    "\n",
    "        # Plot facet data\n",
    "        g = sns.relplot(data=stats_melted,\n",
    "                        x=\"year\", \n",
    "                        y=\"Distance (m)\", \n",
    "                        col=\"id\", \n",
    "                        hue=\"variable\",\n",
    "                        height=1.7, \n",
    "                        aspect=1.0, \n",
    "                        kind=\"line\", \n",
    "                        legend='full', \n",
    "                        col_wrap=5,\n",
    "                        **time_kwargs)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(f'DEA CoastLines vs {title}', \n",
    "                       weight='bold', \n",
    "                       ha='right')\n",
    "        \n",
    "        # Simplify titles\n",
    "        g.set_titles(row_template='{row_name}', \n",
    "                     col_template='{col_name}')\n",
    "\n",
    "        # Customise legend\n",
    "        g._legend.texts[0].set_text(\"\")\n",
    "        g._legend.set_bbox_to_anchor(time_legend_pos)\n",
    "        \n",
    "        # Export to file\n",
    "        g.savefig(f\"{title}_time_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    return pd.Series({'Mean Absolute Error': mae, \n",
    "                    f'RMSE': rmse,\n",
    "                    f'Standard deviation': stdev,\n",
    "                    f'Bias': f'{bias:.1f} m {offset_str}',\n",
    "                    f'Correlation': corr})\n",
    "\n",
    "# for i, sub in stats_subset.groupby('smartline'):\n",
    "\n",
    "#     # Run analysis\n",
    "#     g = val_plot(df=sub,  # stats_subset,\n",
    "#                  title=i.replace('/', '-'),\n",
    "#                  scatter=True, \n",
    "#                  density=False,\n",
    "#                  time=False,\n",
    "#                  time_stat='median',\n",
    "#                  time_legend_pos=[0.67, 0.11],\n",
    "#                  offset=0,\n",
    "#                  extent=(0, 1000))\n",
    "\n",
    "# Run analysis\n",
    "g = val_plot(df=stats_subset,  # stats_subset,\n",
    "         title='nswbpd_eurobodallabeachessouth',\n",
    "         scatter=True, \n",
    "         density=True,\n",
    "         time=False,\n",
    "         time_stat='median',\n",
    "         time_legend_pos=[0.67, 0.11],\n",
    "         offset=0,\n",
    "         extent=(0, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data along profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from scipy import stats\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datacube.utils.geometry import CRS\n",
    "from shapely.geometry import box, shape\n",
    "\n",
    "# Widgets and WMS\n",
    "from odc.ui import ui_poll, select_on_a_map\n",
    "from ipyleaflet import (Map, WMSLayer, WidgetControl, FullScreenControl, \n",
    "                        DrawControl, basemaps, basemap_to_tiles, TileLayer)\n",
    "from ipywidgets.widgets import Layout, Button, HTML\n",
    "from IPython.display import display\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "\n",
    "def extract_geometry(profile,\n",
    "                     start,\n",
    "                     transect_mode='distance'): \n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Convert geometry to a GeoSeries\n",
    "        profile = gpd.GeoSeries(profile, \n",
    "                                crs='EPSG:4326')\n",
    "        start = gpd.GeoSeries(start, \n",
    "                              crs='EPSG:4326').to_crs('EPSG:3577')\n",
    "\n",
    "        # Load data from WFS\n",
    "        xmin, ymin, xmax, ymax = profile.total_bounds\n",
    "        deacl_wfs = f'https://geoserver.dea.ga.gov.au/geoserver/wfs?' \\\n",
    "                    f'service=WFS&version=1.1.0&request=GetFeature' \\\n",
    "                    f'&typeName=dea:coastlines&maxFeatures=1000' \\\n",
    "                    f'&bbox={ymin},{xmin},{ymax},{xmax},' \\\n",
    "                    f'urn:ogc:def:crs:EPSG:4326'\n",
    "        deacl = gpd.read_file(deacl_wfs)\n",
    "        deacl.crs = 'EPSG:3577'\n",
    "\n",
    "        # Raise exception if no coastlines are returned\n",
    "        if len(deacl.index) == 0:\n",
    "            raise ValueError('No annual coastlines were returned for the '\n",
    "                             'supplied transect. Please select another area.')\n",
    "\n",
    "        # Dissolve by year to remove duplicates, then sort by date\n",
    "        deacl = deacl.dissolve(by='year', as_index=False)\n",
    "        deacl['year'] = deacl.year.astype(int)\n",
    "        deacl = deacl.sort_values('year')\n",
    "\n",
    "        # Extract intersections and determine type\n",
    "        profile = profile.to_crs('EPSG:3577')\n",
    "        intersects = deacl.apply(\n",
    "            lambda x: profile.intersection(x.geometry), axis=1)\n",
    "        intersects = gpd.GeoSeries(intersects[0]) \n",
    "\n",
    "        # Select geometry depending on mode\n",
    "        intersects_type = (intersects.type == 'Point' if \n",
    "                           transect_mode == 'distance' else \n",
    "                           intersects.type == 'MultiPoint')\n",
    "\n",
    "        # Remove annual data according to intersections\n",
    "        deacl_filtered = deacl.loc[intersects_type]\n",
    "        drop_years = ', '.join(deacl.year\n",
    "                               .loc[~intersects_type]\n",
    "                               .astype(str)\n",
    "                               .values.tolist())\n",
    "\n",
    "        # In 'distance' mode, analyse years with one intersection only\n",
    "        if transect_mode == 'distance':  \n",
    "\n",
    "            if drop_years:\n",
    "                print(f'Dropping years due to multiple intersections: {drop_years}\\n')            \n",
    "\n",
    "            # Add start and end coordinate\n",
    "            deacl_filtered['start'] = start.iloc[0]\n",
    "            deacl_filtered['end'] = intersects.loc[intersects_type]\n",
    "\n",
    "        # If any data was returned:\n",
    "        if len(deacl_filtered.index) > 0:   \n",
    "\n",
    "            # Compute distance\n",
    "            deacl_filtered['dist'] = deacl_filtered.apply(\n",
    "                lambda x: x.start.distance(x.end), axis=1)\n",
    "\n",
    "            # Extract values\n",
    "            transect_df = pd.DataFrame(deacl_filtered[['year', 'dist']])\n",
    "            transect_df['dist'] = transect_df.dist.round(2)\n",
    "\n",
    "            # Plot data\n",
    "#             fig, ax = plt.subplots(1, 1, figsize=(5, 8))\n",
    "#             transect_df.plot(x='dist', y='year', ax=ax, label='DEA Coastlines')\n",
    "#             ax.set_xlabel(f'{transect_mode.title()} (metres)')\n",
    "\n",
    "            return transect_df.set_index('year')\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_list = []\n",
    "\n",
    "for val_path in val_paths:\n",
    "    \n",
    "    val_df = pd.read_csv(val_path).groupby('id').first()\n",
    "\n",
    "    # Convert validation start and end locations to linestrings\n",
    "    from shapely.geometry import box, Point, LineString\n",
    "    val_geometry = val_df.apply(\n",
    "        lambda x: LineString([(x.end_x, x.end_y), (x.start_x, x.start_y)]), axis=1)\n",
    "\n",
    "    # Convert geometries to GeoDataFrame\n",
    "    val_gdf = gpd.GeoDataFrame(data=val_df,\n",
    "                               geometry=val_geometry,\n",
    "                               crs='EPSG:3577').to_crs('EPSG:4326')\n",
    "\n",
    "    # Get start coord\n",
    "    val_gdf['start_point'] = val_gdf.apply(\n",
    "        lambda x: Point(x.geometry.coords[1]), axis=1)\n",
    "\n",
    "    for i in val_gdf.index:\n",
    "        print(i)\n",
    "        profile_df = extract_geometry(val_gdf.loc[i].geometry, val_gdf.loc[i].start_point)\n",
    "        if profile_df is not None:\n",
    "            profile_list.append(profile_df.rename({'dist': i}, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(profile_list, axis=1).to_csv('fellowes_et_al_2021_profiles_deacoastlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrabeen BPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv('/g/data/r78/DEACoastlines/validation/output_data/wrl_narrabeen.csv', parse_dates=['date'])\n",
    "out = out.set_index('date')\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.query(\"id == 'narrabeen_all_pf8'\")['0_dist'].min()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.query(\"id == 'narrabeen_all_pf8'\")['0_dist'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv('v1.1.4_wrl_narrabeen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.query(\"id == 'narrabeen_all_pf8'\")['deacl_dist'].max() - out.query(\"id == 'narrabeen_all_pf8'\")['deacl_dist'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_var = out.id.unique()[-1]\n",
    "out.loc[out.id == id_var]['0_dist'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** For assistance with any of the Python code or Jupyter Notebooks in this repository, please post a [Github issue](https://github.com/GeoscienceAustralia/DEACoastLines/issues/new). For questions or more information about this product, sign up to the [Open Data Cube Slack](https://join.slack.com/t/opendatacube/shared_invite/zt-d6hu7l35-CGDhSxiSmTwacKNuXWFUkg) and post on the [`#dea-coastlines`](https://app.slack.com/client/T0L4V0TFT/C018X6J9HLY/details/) channel.\n",
    "\n",
    "**Last modified:** March 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
